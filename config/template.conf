
#   # path = ["/usr/share/snmp/mibs"]
#   ##
#   ## Timeout running snmptranslate command
#   ## Used by the netsnmp translator only
#   # timeout = "5s"
#   ## Snmp version; one of "1", "2c" or "3".
#   # version = "2c"
#   ## SNMPv3 authentication and encryption options.
#   ##
#   ## Security Name.
#   # sec_name = "myuser"
#   ## Authentication protocol; one of "MD5", "SHA", "SHA224", "SHA256", "SHA384", "SHA512" or "".
#   # auth_protocol = "MD5"
#   ## Authentication password.
#   # auth_password = "pass"
#   ## Security Level; one of "noAuthNoPriv", "authNoPriv", or "authPriv".
#   # sec_level = "authNoPriv"
#   ## Privacy protocol used for encrypted messages; one of "DES", "AES", "AES192", "AES192C", "AES256", "AES256C" or "".
#   # priv_protocol = ""
#   ## Privacy password used for encrypted messages.
#   # priv_password = ""


# # Generic socket listener capable of handling multiple socket types.
# [[inputs.socket_listener]]
#   ## URL to listen on
#   # service_address = "tcp://:8094"
#   # service_address = "tcp://127.0.0.1:http"
#   # service_address = "tcp4://:8094"
#   # service_address = "tcp6://:8094"
#   # service_address = "tcp6://[2001:db8::1]:8094"
#   # service_address = "udp://:8094"
#   # service_address = "udp4://:8094"
#   # service_address = "udp6://:8094"
#   # service_address = "unix:///tmp/telegraf.sock"
#   # service_address = "unixgram:///tmp/telegraf.sock"
#   # service_address = "vsock://cid:port"
#
#   ## Permission for unix sockets (only available on unix sockets)
#   ## This setting may not be respected by some platforms. To safely restrict
#   ## permissions it is recommended to place the socket into a previously
#   ## created directory with the desired permissions.
#   ##   ex: socket_mode = "777"
#   # socket_mode = ""
#
#   ## Maximum number of concurrent connections (only available on stream sockets like TCP)
#   ## Zero means unlimited.
#   # max_connections = 0
#
#   ## Read timeout (only available on stream sockets like TCP)
#   ## Zero means unlimited.
#   # read_timeout = "0s"
#
#   ## Optional TLS configuration (only available on stream sockets like TCP)
#   # tls_cert = "/etc/telegraf/cert.pem"
#   # tls_key  = "/etc/telegraf/key.pem"
#   ## Enables client authentication if set.
#   # tls_allowed_cacerts = ["/etc/telegraf/clientca.pem"]
#
#   ## Maximum socket buffer size (in bytes when no unit specified)
#   ## For stream sockets, once the buffer fills up, the sender will start
#   ## backing up. For datagram sockets, once the buffer fills up, metrics will
#   ## start dropping. Defaults to the OS default.
#   # read_buffer_size = "64KiB"
#
#   ## Period between keep alive probes (only applies to TCP sockets)
#   ## Zero disables keep alive probes. Defaults to the OS configuration.
#   # keep_alive_period = "5m"
#
#   ## Content encoding for message payloads
#   ## Can be set to "gzip" for compressed payloads or "identity" for no encoding.
#   # content_encoding = "identity"
#
#   ## Maximum size of decoded packet (in bytes when no unit specified)
#   # max_decompression_size = "500MB"
#
#   ## List of allowed source IP addresses for incoming packets/messages.
#   ## If not specified or empty, all sources are allowed.
#   # allowed_sources = []
#
#   ## Message splitting strategy and corresponding settings for stream sockets
#   ## (tcp, tcp4, tcp6, unix or unixpacket). The setting is ignored for packet
#   ## listeners such as udp.
#   ## Available strategies are:
#   ##   newline         -- split at newlines (default)
#   ##   null            -- split at null bytes
#   ##   delimiter       -- split at delimiter byte-sequence in hex-format
#   ##                      given in `splitting_delimiter`
#   ##   fixed length    -- split after number of bytes given in `splitting_length`
#   ##   variable length -- split depending on length information received in the
#   ##                      data. The length field information is specified in
#   ##                      `splitting_length_field`.
#   # splitting_strategy = "newline"
#
#   ## Delimiter used to split received data to messages consumed by the parser.
#   ## The delimiter is a hex byte-sequence marking the end of a message
#   ## e.g. "0x0D0A", "x0d0a" or "0d0a" marks a Windows line-break (CR LF).
#   ## The value is case-insensitive and can be specified with "0x" or "x" prefix
#   ## or without.
#   ## Note: This setting is only used for splitting_strategy = "delimiter".
#   # splitting_delimiter = ""
#
#   ## Fixed length of a message in bytes.
#   ## Note: This setting is only used for splitting_strategy = "fixed length".
#   # splitting_length = 0
#
#   ## Specification of the length field contained in the data to split messages
#   ## with variable length. The specification contains the following fields:
#   ##  offset        -- start of length field in bytes from begin of data
#   ##  bytes         -- length of length field in bytes
#   ##  endianness    -- endianness of the value, either "be" for big endian or
#   ##                   "le" for little endian
#   ##  header_length -- total length of header to be skipped when passing
#   ##                   data on to the parser. If zero (default), the header
#   ##                   is passed on to the parser together with the message.
#   ## Note: This setting is only used for splitting_strategy = "variable length".
#   # splitting_length_field = {offset = 0, bytes = 0, endianness = "be", header_length = 0}
#
#   ## Data format to consume.
#   ## Each data format has its own unique set of configuration options, read
#   ## more about them here:
#   ## https://github.com/influxdata/telegraf/blob/master/docs/DATA_FORMATS_INPUT.md
#   # data_format = "influx"


# # Read stats from one or more Solr servers or cores
# [[inputs.solr]]
#   ## specify a list of one or more Solr servers
#   servers = ["http://localhost:8983"]
#
#   ## specify a list of one or more Solr cores (default - all)
#   # cores = ["*"]
#
#   ## Optional HTTP Basic Auth Credentials
#   # username = "username"
#   # password = "pa$$word"
#
#   ## Timeout for HTTP requests
#   # timeout = "5s"


# # Read metrics from SQL queries
# [[inputs.sql]]
#   ## Database Driver
#   ## See https://github.com/influxdata/telegraf/blob/master/docs/SQL_DRIVERS_INPUT.md for
#   ## a list of supported drivers.
#   driver = "mysql"
#
#   ## Data source name for connecting
#   ## The syntax and supported options depends on selected driver.
#   dsn = "username:password@tcp(mysqlserver:3307)/dbname?param=value"
#
#   ## Timeout for any operation
#   ## Note that the timeout for queries is per query not per gather.
#   # timeout = "5s"
#
#   ## Connection time limits
#   ## By default the maximum idle time and maximum lifetime of a connection is unlimited.
#   ## Connections will not be closed automatically. If you specify a positive time, the connections will be closed after
#   ## idling or existing for at least that amount of time, respectively.
#   # connection_max_idle_time = "0s"
#   # connection_max_life_time = "0s"
#
#   ## Connection count limits
#   ## By default the number of open connections is not limited and the number of maximum idle connections
#   ## will be inferred from the number of queries specified. If you specify a positive number for any of the
#   ## two options, connections will be closed when reaching the specified limit. The number of idle connections
#   ## will be clipped to the maximum number of connections limit if any.
#   # connection_max_open = 0
#   # connection_max_idle = auto
#
#   ## Specifies plugin behavior regarding disconnected servers
#   ## Available choices :
#   ##   - error: telegraf will return an error on startup if one the servers is unreachable
#   ##   - ignore: telegraf will ignore unreachable servers on both startup and gather
#   # disconnected_servers_behavior = "error"
#
#   [[inputs.sql.query]]
#     ## Query to perform on the server
#     query="SELECT user,state,latency,score FROM Scoreboard WHERE application > 0"
#     ## Alternatively to specifying the query directly you can select a file here containing the SQL query.
#     ## Only one of 'query' and 'query_script' can be specified!
#     # query_script = "/path/to/sql/script.sql"
#
#     ## Name of the measurement
#     ## In case both measurement and 'measurement_col' are given, the latter takes precedence.
#     # measurement = "sql"
#
#     ## Column name containing the name of the measurement
#     ## If given, this will take precedence over the 'measurement' setting. In case a query result
#     ## does not contain the specified column, we fall-back to the 'measurement' setting.
#     # measurement_column = ""
#
#     ## Column name containing the time of the measurement
#     ## If omitted, the time of the query will be used.
#     # time_column = ""
#
#     ## Format of the time contained in 'time_col'
#     ## The time must be 'unix', 'unix_ms', 'unix_us', 'unix_ns', or a golang time format.
#     ## See https://golang.org/pkg/time/#Time.Format for details.
#     # time_format = "unix"
#
#     ## Column names containing tags
#     ## An empty include list will reject all columns and an empty exclude list will not exclude any column.
#     ## I.e. by default no columns will be returned as tag and the tags are empty.
#     # tag_columns_include = []
#     # tag_columns_exclude = []
#
#     ## Column names containing fields (explicit types)
#     ## Convert the given columns to the corresponding type. Explicit type conversions take precedence over
#     ## the automatic (driver-based) conversion below.
#     ## NOTE: Columns should not be specified for multiple types or the resulting type is undefined.
#     # field_columns_float = []
#     # field_columns_int = []
#     # field_columns_uint = []
#     # field_columns_bool = []
#     # field_columns_string = []
#
#     ## Column names containing fields (automatic types)
#     ## An empty include list is equivalent to '[*]' and all returned columns will be accepted. An empty
#     ## exclude list will not exclude any column. I.e. by default all columns will be returned as fields.
#     ## NOTE: We rely on the database driver to perform automatic datatype conversion.
#     # field_columns_include = []
#     # field_columns_exclude = []


# # Read metrics from Microsoft SQL Server
# [[inputs.sqlserver]]
#   ## Specify instances to monitor with a list of connection strings.
#   ## All connection parameters are optional.
#   ## By default, the host is localhost, listening on default port, TCP 1433.
#   ##   for Windows, the user is the currently running AD user (SSO).
#   ##   See https://github.com/microsoft/go-mssqldb for detailed connection
#   ##   parameters, in particular, tls connections can be created like so:
#   ##   "encrypt=true;certificate=<cert>;hostNameInCertificate=<SqlServer host fqdn>"
#   servers = [
#     "Server=192.168.1.10;Port=1433;User Id=<user>;Password=<pw>;app name=telegraf;log=1;",
#   ]
#
#   ## Timeout for query execution operation
#   ## Note that the timeout for queries is per query not per gather.
#   ## 0 value means no timeout
#   # query_timeout = "0s"
#
#   ## Authentication method
#   ## valid methods: "connection_string", "AAD"
#   # auth_method = "connection_string"
#
#   ## ClientID is the is the client ID of the user assigned identity of the VM
#   ## that should be used to authenticate to the Azure SQL server.
#   # client_id = ""
#
#   ## "database_type" enables a specific set of queries depending on the database type. If specified, it replaces azuredb = true/false and query_version = 2
#   ## In the config file, the sql server plugin section should be repeated each with a set of servers for a specific database_type.
#   ## Possible values for database_type are - "SQLServer" or "AzureSQLDB" or "AzureSQLManagedInstance" or "AzureSQLPool"
#   database_type = "SQLServer"
#
#   ## A list of queries to include. If not specified, all the below listed queries are used.
#   include_query = []
#
#   ## A list of queries to explicitly ignore.
#   exclude_query = ["SQLServerAvailabilityReplicaStates", "SQLServerDatabaseReplicaStates"]
#
#   ## Force using the deprecated ADAL authentication method instead of the recommended
#   ## MSAL method. Setting this option is not recommended and only exists for backward
#   ## compatibility.
#   # use_deprecated_adal_authentication = false
#
#   ## Queries enabled by default for database_type = "SQLServer" are -
#   ## SQLServerPerformanceCounters, SQLServerWaitStatsCategorized, SQLServerDatabaseIO, SQLServerProperties, SQLServerMemoryClerks,
#   ## SQLServerSchedulers, SQLServerRequests, SQLServerVolumeSpace, SQLServerCpu, SQLServerAvailabilityReplicaStates, SQLServerDatabaseReplicaStates,
#   ## SQLServerRecentBackups
#
#   ## Queries enabled by default for database_type = "AzureSQLDB" are -
#   ## AzureSQLDBResourceStats, AzureSQLDBResourceGovernance, AzureSQLDBWaitStats, AzureSQLDBDatabaseIO, AzureSQLDBServerProperties,
#   ## AzureSQLDBOsWaitstats, AzureSQLDBMemoryClerks, AzureSQLDBPerformanceCounters, AzureSQLDBRequests, AzureSQLDBSchedulers
#
#   ## Queries enabled by default for database_type = "AzureSQLManagedInstance" are -
#   ## AzureSQLMIResourceStats, AzureSQLMIResourceGovernance, AzureSQLMIDatabaseIO, AzureSQLMIServerProperties, AzureSQLMIOsWaitstats,
#   ## AzureSQLMIMemoryClerks, AzureSQLMIPerformanceCounters, AzureSQLMIRequests, AzureSQLMISchedulers
#
#   ## Queries enabled by default for database_type = "AzureSQLPool" are -
#   ## AzureSQLPoolResourceStats, AzureSQLPoolResourceGovernance, AzureSQLPoolDatabaseIO, AzureSQLPoolWaitStats,
#   ## AzureSQLPoolMemoryClerks, AzureSQLPoolPerformanceCounters, AzureSQLPoolSchedulers
#
#   ## Queries enabled by default for database_type = "AzureArcSQLManagedInstance" are -
#   ## AzureSQLMIDatabaseIO, AzureSQLMIServerProperties, AzureSQLMIOsWaitstats,
#   ## AzureSQLMIMemoryClerks, AzureSQLMIPerformanceCounters, AzureSQLMIRequests, AzureSQLMISchedulers
#
#   ## Following are old config settings
#   ## You may use them only if you are using the earlier flavor of queries, however it is recommended to use
#   ## the new mechanism of identifying the database_type there by use it's corresponding queries
#
#   ## Optional parameter, setting this to 2 will use a new version
#   ## of the collection queries that break compatibility with the original
#   ## dashboards.
#   ## Version 2 - is compatible from SQL Server 2012 and later versions and also for SQL Azure DB
#   # query_version = 2
#
#   ## If you are using AzureDB, setting this to true will gather resource utilization metrics
#   # azuredb = false
#
#   ## Toggling this to true will emit an additional metric called "sqlserver_telegraf_health".
#   ## This metric tracks the count of attempted queries and successful queries for each SQL instance specified in "servers".
#   ## The purpose of this metric is to assist with identifying and diagnosing any connectivity or query issues.
#   ## This setting/metric is optional and is disabled by default.
#   # health_metric = false
#
#   ## Possible queries across different versions of the collectors
#   ## Queries enabled by default for specific Database Type
#
#   ## database_type =  AzureSQLDB  by default collects the following queries
#   ## - AzureSQLDBWaitStats
#   ## - AzureSQLDBResourceStats
#   ## - AzureSQLDBResourceGovernance
#   ## - AzureSQLDBDatabaseIO
#   ## - AzureSQLDBServerProperties
#   ## - AzureSQLDBOsWaitstats
#   ## - AzureSQLDBMemoryClerks
#   ## - AzureSQLDBPerformanceCounters
#   ## - AzureSQLDBRequests
#   ## - AzureSQLDBSchedulers
#
#   ## database_type =  AzureSQLManagedInstance by default collects the following queries
#   ## - AzureSQLMIResourceStats
#   ## - AzureSQLMIResourceGovernance
#   ## - AzureSQLMIDatabaseIO
#   ## - AzureSQLMIServerProperties
#   ## - AzureSQLMIOsWaitstats
#   ## - AzureSQLMIMemoryClerks
#   ## - AzureSQLMIPerformanceCounters
#   ## - AzureSQLMIRequests
#   ## - AzureSQLMISchedulers
#
#   ## database_type =  AzureSQLPool by default collects the following queries
#   ## - AzureSQLPoolResourceStats
#   ## - AzureSQLPoolResourceGovernance
#   ## - AzureSQLPoolDatabaseIO
#   ## - AzureSQLPoolOsWaitStats,
#   ## - AzureSQLPoolMemoryClerks
#   ## - AzureSQLPoolPerformanceCounters
#   ## - AzureSQLPoolSchedulers
#
#   ## database_type =  SQLServer by default collects the following queries
#   ## - SQLServerPerformanceCounters
#   ## - SQLServerWaitStatsCategorized
#   ## - SQLServerDatabaseIO
#   ## - SQLServerProperties
#   ## - SQLServerMemoryClerks
#   ## - SQLServerSchedulers
#   ## - SQLServerRequests
#   ## - SQLServerVolumeSpace
#   ## - SQLServerCpu
#   ## - SQLServerRecentBackups
#   ## and following as optional (if mentioned in the include_query list)
#   ## - SQLServerAvailabilityReplicaStates
#   ## - SQLServerDatabaseReplicaStates
#
#   ## Maximum number of open connections to the database, 0 allows the driver to decide.
#   # max_open_connections = 0
#
#   ## Maximum number of idle connections in the connection pool, 0 allows the driver to decide.
#   # max_idle_connections = 0


# # Statsd Server
# [[inputs.statsd]]
#   ## Protocol, must be "tcp", "udp4", "udp6" or "udp" (default=udp)
#   protocol = "udp"
#
#   ## MaxTCPConnection - applicable when protocol is set to tcp (default=250)
#   max_tcp_connections = 250
#
#   ## Enable TCP keep alive probes (default=false)
#   tcp_keep_alive = false
#
#   ## Specifies the keep-alive period for an active network connection.
#   ## Only applies to TCP sockets and will be ignored if tcp_keep_alive is false.
#   ## Defaults to the OS configuration.
#   # tcp_keep_alive_period = "2h"
#
#   ## Address and port to host UDP listener on
#   service_address = ":8125"
#
#   ## The following configuration options control when telegraf clears it's cache
#   ## of previous values. If set to false, then telegraf will only clear it's
#   ## cache when the daemon is restarted.
#   ## Reset gauges every interval (default=true)
#   delete_gauges = true
#   ## Reset counters every interval (default=true)
#   delete_counters = true
#   ## Reset sets every interval (default=true)
#   delete_sets = true
#   ## Reset timings & histograms every interval (default=true)
#   delete_timings = true
#
#   ## Enable aggregation temporality adds temporality=delta or temporality=commulative tag, and
#   ## start_time field, which adds the start time of the metric accumulation.
#   ## You should use this when using OpenTelemetry output.
#   # enable_aggregation_temporality = false
#
#   ## Percentiles to calculate for timing & histogram stats.
#   percentiles = [50.0, 90.0, 99.0, 99.9, 99.95, 100.0]
#
#   ## separator to use between elements of a statsd metric
#   metric_separator = "_"
#
#   ## Parses extensions to statsd in the datadog statsd format
#   ## currently supports metrics and datadog tags.
#   ## http://docs.datadoghq.com/guides/dogstatsd/
#   datadog_extensions = false
#
#   ## Parses distributions metric as specified in the datadog statsd format
#   ## https://docs.datadoghq.com/developers/metrics/types/?tab=distribution#definition
#   datadog_distributions = false
#
#   ## Keep or drop the container id as tag. Included as optional field
#   ## in DogStatsD protocol v1.2 if source is running in Kubernetes
#   ## https://docs.datadoghq.com/developers/dogstatsd/datagram_shell/?tab=metrics#dogstatsd-protocol-v12
#   datadog_keep_container_tag = false
#
#   ## Statsd data translation templates, more info can be read here:
#   ## https://github.com/influxdata/telegraf/blob/master/docs/TEMPLATE_PATTERN.md
#   # templates = [
#   #     "cpu.* measurement*"
#   # ]
#
#   ## Number of UDP messages allowed to queue up, once filled,
#   ## the statsd server will start dropping packets
#   allowed_pending_messages = 10000
#
#   ## Number of worker threads used to parse the incoming messages.
#   # number_workers_threads = 5
#
#   ## Number of timing/histogram values to track per-measurement in the
#   ## calculation of percentiles. Raising this limit increases the accuracy
#   ## of percentiles but also increases the memory usage and cpu time.
#   percentile_limit = 1000
#
#   ## Maximum socket buffer size in bytes, once the buffer fills up, metrics
#   ## will start dropping.  Defaults to the OS default.
#   # read_buffer_size = 65535
#
#   ## Max duration (TTL) for each metric to stay cached/reported without being updated.
#   # max_ttl = "10h"
#
#   ## Sanitize name method
#   ## By default, telegraf will pass names directly as they are received.
#   ## However, upstream statsd now does sanitization of names which can be
#   ## enabled by using the "upstream" method option. This option will a) replace
#   ## white space with '_', replace '/' with '-', and remove characters not
#   ## matching 'a-zA-Z_\-0-9\.;='.
#   #sanitize_name_method = ""
#
#   ## Replace dots (.) with underscore (_) and dashes (-) with
#   ## double underscore (__) in metric names.
#   # convert_names = false
#
#   ## Convert all numeric counters to float
#   ## Enabling this would ensure that both counters and guages are both emitted
#   ## as floats.
#   # float_counters = false
#
#   ## Emit timings `metric_<name>_count` field as float, the same as all other
#   ## histogram fields
#   # float_timings = false
#
#   ## Emit sets as float
#   # float_sets = false


# # Suricata stats and alerts plugin
# [[inputs.suricata]]
#   ## Source
#   ## Data sink for Suricata stats log. This is expected to be a filename of a
#   ## unix socket to be created for listening.
#   # source = "/var/run/suricata-stats.sock"
#
#   ## Delimiter
#   ## Used for flattening field keys, e.g. subitem "alert" of "detect" becomes
#   ## "detect_alert" when delimiter is "_".
#   # delimiter = "_"
#
#   ## Metric version
#   ## Version 1 only collects stats and optionally will look for alerts if
#   ## the configuration setting alerts is set to true.
#   ## Version 2 parses any event type message by default and produced metrics
#   ## under a single metric name using a tag to differentiate between event
#   ## types. The timestamp for the message is applied to the generated metric.
#   ## Additional tags and fields are included as well.
#   # version = "1"
#
#   ## Alerts
#   ## In metric version 1, only status is captured by default, alerts must be
#   ## turned on with this configuration option. This option does not apply for
#   ## metric version 2.
#   # alerts = false


# [[inputs.syslog]]
#   ## Protocol, address and port to host the syslog receiver.
#   ## If no host is specified, then localhost is used.
#   ## If no port is specified, 6514 is used (RFC5425#section-4.1).
#   ##   ex: server = "tcp://localhost:6514"
#   ##       server = "udp://:6514"
#   ##       server = "unix:///var/run/telegraf-syslog.sock"
#   ## When using tcp, consider using 'tcp4' or 'tcp6' to force the usage of IPv4
#   ## or IPV6 respectively. There are cases, where when not specified, a system
#   ## may force an IPv4 mapped IPv6 address.
#   server = "tcp://127.0.0.1:6514"
#
#   ## Permission for unix sockets (only available on unix sockets)
#   ## This setting may not be respected by some platforms. To safely restrict
#   ## permissions it is recommended to place the socket into a previously
#   ## created directory with the desired permissions.
#   ##   ex: socket_mode = "777"
#   # socket_mode = ""
#
#   ## Maximum number of concurrent connections (only available on stream sockets like TCP)
#   ## Zero means unlimited.
#   # max_connections = 0
#
#   ## Read timeout (only available on stream sockets like TCP)
#   ## Zero means unlimited.
#   # read_timeout = "0s"
#
#   ## Optional TLS configuration (only available on stream sockets like TCP)
#   # tls_cert = "/etc/telegraf/cert.pem"
#   # tls_key  = "/etc/telegraf/key.pem"
#   ## Enables client authentication if set.
#   # tls_allowed_cacerts = ["/etc/telegraf/clientca.pem"]
#
#   ## Maximum socket buffer size (in bytes when no unit specified)
#   ## For stream sockets, once the buffer fills up, the sender will start
#   ## backing up. For datagram sockets, once the buffer fills up, metrics will
#   ## start dropping. Defaults to the OS default.
#   # read_buffer_size = "64KiB"
#
#   ## Period between keep alive probes (only applies to TCP sockets)
#   ## Zero disables keep alive probes. Defaults to the OS configuration.
#   # keep_alive_period = "5m"
#
#   ## Content encoding for message payloads
#   ## Can be set to "gzip" for compressed payloads or "identity" for no encoding.
#   # content_encoding = "identity"
#
#   ## Maximum size of decoded packet (in bytes when no unit specified)
#   # max_decompression_size = "500MB"
#
#   ## List of allowed source IP addresses for incoming packets/messages.
#   ## If not specified or empty, all sources are allowed.
#   # allowed_sources = []
#
#   ## Framing technique used for messages transport
#   ## Available settings are:
#   ##   octet-counting  -- see RFC5425#section-4.3.1 and RFC6587#section-3.4.1
#   ##   non-transparent -- see RFC6587#section-3.4.2
#   # framing = "octet-counting"
#
#   ## The trailer to be expected in case of non-transparent framing (default = "LF").
#   ## Must be one of "LF", or "NUL".
#   # trailer = "LF"
#
#   ## Whether to parse in best effort mode or not (default = false).
#   ## By default best effort parsing is off.
#   # best_effort = false
#
#   ## The RFC standard to use for message parsing
#   ## By default RFC5424 is used. RFC3164 only supports UDP transport (no streaming support)
#   ## Must be one of "RFC5424", or "RFC3164".
#   # syslog_standard = "RFC5424"
#
#   ## Character to prepend to SD-PARAMs (default = "_").
#   ## A syslog message can contain multiple parameters and multiple identifiers within structured data section.
#   ## Eg., [id1 name1="val1" name2="val2"][id2 name1="val1" nameA="valA"]
#   ## For each combination a field is created.
#   ## Its name is created concatenating identifier, sdparam_separator, and parameter name.
#   # sdparam_separator = "_"
#
#   ## Maximum length allowed for a single message (in bytes when no unit specified)
#   ## Only applies to octet-counting framing.
#   # max_message_length = "8KiB"


# # Gather information about systemd-unit states
# # This plugin ONLY supports Linux
# [[inputs.systemd_units]]
#   ## Pattern of units to collect
#   ## A space-separated list of unit-patterns including wildcards determining
#   ## the units to collect.
#   ##  ex: pattern = "telegraf* influxdb* user@*"
#   # pattern = "*"
#
#   ## Filter for a specific unit type
#   ## Available settings are: service, socket, target, device, mount,
#   ## automount, swap, timer, path, slice and scope
#   # unittype = "service"
#
#   ## Collect system or user scoped units
#   ##  ex: scope = "user"
#   # scope = "system"
#
#   ## Collect also units not loaded by systemd (disabled or static units)
#   ## Enabling this feature might introduce significant load when used with
#   ## unspecific patterns (such as '*') as systemd will need to load all
#   ## matching unit files.
#   # collect_disabled_units = false
#
#   ## Collect detailed information for the units
#   # details = false
#
#   ## Timeout for state-collection
#   # timeout = "5s"


# # Parse the new lines appended to a file
# [[inputs.tail]]
#   ## File names or a pattern to tail.
#   ## These accept standard unix glob matching rules, but with the addition of
#   ## ** as a "super asterisk". ie:
#   ##   "/var/log/**.log"  -> recursively find all .log files in /var/log
#   ##   "/var/log/*/*.log" -> find all .log files with a parent dir in /var/log
#   ##   "/var/log/apache.log" -> just tail the apache log file
#   ##   "/var/log/log[!1-2]*  -> tail files without 1-2
#   ##   "/var/log/log[^1-2]*  -> identical behavior as above
#   ## See https://github.com/gobwas/glob for more examples
#   ##
#   files = ["/var/mymetrics.out"]
#
#   ## Offset to start reading at
#   ## The following methods are available:
#   ##   beginning          -- start reading from the beginning of the file ignoring any persisted offset
#   ##   end                -- start reading from the end of the file ignoring any persisted offset
#   ##   saved-or-beginning -- use the persisted offset of the file or, if no offset persisted, start from the beginning of the file
#   ##   saved-or-end       -- use the persisted offset of the file or, if no offset persisted, start from the end of the file
#   # initial_read_offset = "saved-or-end"
#
#   ## Whether file is a named pipe
#   # pipe = false
#
#   ## Method used to watch for file updates.  Can be either "inotify" or "poll".
#   ## inotify is supported on linux, *bsd, and macOS, while Windows requires
#   ## using poll. Poll checks for changes every 250ms.
#   # watch_method = "inotify"
#
#   ## Maximum lines of the file to process that have not yet be written by the
#   ## output.  For best throughput set based on the number of metrics on each
#   ## line and the size of the output's metric_batch_size.
#   # max_undelivered_lines = 1000
#
#   ## Character encoding to use when interpreting the file contents.  Invalid
#   ## characters are replaced using the unicode replacement character.  When set
#   ## to the empty string the data is not decoded to text.
#   ##   ex: character_encoding = "utf-8"
#   ##       character_encoding = "utf-16le"
#   ##       character_encoding = "utf-16be"
#   ##       character_encoding = ""
#   # character_encoding = ""
#
#   ## Data format to consume.
#   ## Each data format has its own unique set of configuration options, read
#   ## more about them here:
#   ## https://github.com/influxdata/telegraf/blob/master/docs/DATA_FORMATS_INPUT.md
#   data_format = "influx"
#
#   ## Set the tag that will contain the path of the tailed file. If you don't want this tag, set it to an empty string.
#   # path_tag = "path"
#
#   ## Filters to apply to files before generating metrics
#   ## "ansi_color" removes ANSI colors
#   # filters = []
#
#   ## multiline parser/codec
#   ## https://www.elastic.co/guide/en/logstash/2.4/plugins-filters-multiline.html
#   #[inputs.tail.multiline]
#     ## The pattern should be a regexp which matches what you believe to be an indicator that the field is part of an event consisting of multiple lines of log data.
#     #pattern = "^\s"
#
#     ## The field's value must be previous or next and indicates the relation to the
#     ## multi-line event.
#     #match_which_line = "previous"
#
#     ## The invert_match can be true or false (defaults to false).
#     ## If true, a message not matching the pattern will constitute a match of the multiline filter and the what will be applied. (vice-versa is also true)
#     #invert_match = false
#
#     ## The handling method for quoted text (defaults to 'ignore').
#     ## The following methods are available:
#     ##   ignore  -- do not consider quotation (default)
#     ##   single-quotes -- consider text quoted by single quotes (')
#     ##   double-quotes -- consider text quoted by double quotes (")
#     ##   backticks     -- consider text quoted by backticks (`)
#     ## When handling quotes, escaped quotes (e.g. \") are handled correctly.
#     #quotation = "ignore"
#
#     ## The preserve_newline option can be true or false (defaults to false).
#     ## If true, the newline character is preserved for multiline elements,
#     ## this is useful to preserve message-structure e.g. for logging outputs.
#     #preserve_newline = false
#
#     #After the specified timeout, this plugin sends the multiline event even if no new pattern is found to start a new event. The default is 5s.
#     #timeout = 5s


# ## Gather CPU metrics using Turbostat
# [[inputs.turbostat]]
#   ## Path to the Turbostat exectuable if not in the PATH
#   # path = "/usr/bin/turbostat"
#
#   ## Turbostat measurement interval
#   # interval = "10s"
#
#   ## Use sudo to run the Turbostat executable
#   # use_sudo = false


# # Read metrics from the Vault API
# [[inputs.vault]]
#   ## URL for the Vault agent
#   # url = "http://127.0.0.1:8200"
#
#   ## Use Vault token for authorization.
#   ## Vault token configuration is mandatory.
#   ## If both are empty or both are set, an error is thrown.
#   # token_file = "/path/to/auth/token"
#   ## OR
#   token = "s.CDDrgg5zPv5ssI0Z2P4qxJj2"
#
#   ## Set response_timeout (default 5 seconds)
#   # response_timeout = "5s"
#
#   ## Optional TLS Config
#   # tls_ca = /path/to/cafile
#   # tls_cert = /path/to/certfile
#   # tls_key = /path/to/keyfile


# # Read metrics from one or many vCenters
# [[inputs.vsphere]]
#   ## List of vCenter URLs to be monitored. These three lines must be uncommented
#   ## and edited for the plugin to work.
#   vcenters = [ "https://vcenter.local/sdk" ]
#   username = "user@corp.local"
#   password = "secret"
#
#   ## VMs
#   ## Typical VM metrics (if omitted or empty, all metrics are collected)
#   # vm_include = [ "/*/vm/**"] # Inventory path to VMs to collect (by default all are collected)
#   # vm_exclude = [] # Inventory paths to exclude
#   vm_metric_include = [
#     "cpu.demand.average",
#     "cpu.idle.summation",
#     "cpu.latency.average",
#     "cpu.readiness.average",
#     "cpu.ready.summation",
#     "cpu.run.summation",
#     "cpu.usagemhz.average",
#     "cpu.used.summation",
#     "cpu.wait.summation",
#     "mem.active.average",
#     "mem.granted.average",
#     "mem.latency.average",
#     "mem.swapin.average",
#     "mem.swapinRate.average",
#     "mem.swapout.average",
#     "mem.swapoutRate.average",
#     "mem.usage.average",
#     "mem.vmmemctl.average",
#     "net.bytesRx.average",
#     "net.bytesTx.average",
#     "net.droppedRx.summation",
#     "net.droppedTx.summation",
#     "net.usage.average",
#     "power.power.average",
#     "virtualDisk.numberReadAveraged.average",
#     "virtualDisk.numberWriteAveraged.average",
#     "virtualDisk.read.average",
#     "virtualDisk.readOIO.latest",
#     "virtualDisk.throughput.usage.average",
#     "virtualDisk.totalReadLatency.average",
#     "virtualDisk.totalWriteLatency.average",
#     "virtualDisk.write.average",
#     "virtualDisk.writeOIO.latest",
#     "sys.uptime.latest",
#   ]
#   # vm_metric_exclude = [] ## Nothing is excluded by default
#   # vm_instances = true ## true by default
#
#   ## Hosts
#   ## Typical host metrics (if omitted or empty, all metrics are collected)
#   # host_include = [ "/*/host/**"] # Inventory path to hosts to collect (by default all are collected)
#   # host_exclude [] # Inventory paths to exclude
#   host_metric_include = [
#     "cpu.coreUtilization.average",
#     "cpu.costop.summation",
#     "cpu.demand.average",
#     "cpu.idle.summation",
#     "cpu.latency.average",
#     "cpu.readiness.average",
#     "cpu.ready.summation",
#     "cpu.swapwait.summation",
#     "cpu.usage.average",
#     "cpu.usagemhz.average",
#     "cpu.used.summation",
#     "cpu.utilization.average",
#     "cpu.wait.summation",
#     "disk.deviceReadLatency.average",
#     "disk.deviceWriteLatency.average",
#     "disk.kernelReadLatency.average",
#     "disk.kernelWriteLatency.average",
#     "disk.numberReadAveraged.average",
#     "disk.numberWriteAveraged.average",
#     "disk.read.average",
#     "disk.totalReadLatency.average",
#     "disk.totalWriteLatency.average",
#     "disk.write.average",
#     "mem.active.average",
#     "mem.latency.average",
#     "mem.state.latest",
#     "mem.swapin.average",
#     "mem.swapinRate.average",
#     "mem.swapout.average",
#     "mem.swapoutRate.average",
#     "mem.totalCapacity.average",
#     "mem.usage.average",
#     "mem.vmmemctl.average",
#     "net.bytesRx.average",
#     "net.bytesTx.average",
#     "net.droppedRx.summation",
#     "net.droppedTx.summation",
#     "net.errorsRx.summation",
#     "net.errorsTx.summation",
#     "net.usage.average",
#     "power.power.average",
#     "storageAdapter.numberReadAveraged.average",
#     "storageAdapter.numberWriteAveraged.average",
#     "storageAdapter.read.average",
#     "storageAdapter.write.average",
#     "sys.uptime.latest",
#   ]
#     ## Collect IP addresses? Valid values are "ipv4" and "ipv6"
#   # ip_addresses = ["ipv6", "ipv4" ]
#
#   # host_metric_exclude = [] ## Nothing excluded by default
#   # host_instances = true ## true by default
#
#
#   ## Clusters
#   # cluster_include = [ "/*/host/**"] # Inventory path to clusters to collect (by default all are collected)
#   # cluster_exclude = [] # Inventory paths to exclude
#   # cluster_metric_include = [] ## if omitted or empty, all metrics are collected
#   # cluster_metric_exclude = [] ## Nothing excluded by default
#   # cluster_instances = false ## false by default
#
#   ## Resource Pools
#   # resource_pool_include = [ "/*/host/**"] # Inventory path to resource pools to collect (by default all are collected)
#   # resource_pool_exclude = [] # Inventory paths to exclude
#   # resource_pool_metric_include = [] ## if omitted or empty, all metrics are collected
#   # resource_pool_metric_exclude = [] ## Nothing excluded by default
#   # resource_pool_instances = false ## false by default
#
#   ## Datastores
#   # datastore_include = [ "/*/datastore/**"] # Inventory path to datastores to collect (by default all are collected)
#   # datastore_exclude = [] # Inventory paths to exclude
#   # datastore_metric_include = [] ## if omitted or empty, all metrics are collected
#   # datastore_metric_exclude = [] ## Nothing excluded by default
#   # datastore_instances = false ## false by default
#
#   ## Datacenters
#   # datacenter_include = [ "/*/host/**"] # Inventory path to clusters to collect (by default all are collected)
#   # datacenter_exclude = [] # Inventory paths to exclude
#   datacenter_metric_include = [] ## if omitted or empty, all metrics are collected
#   datacenter_metric_exclude = [ "*" ] ## Datacenters are not collected by default.
#   # datacenter_instances = false ## false by default
#
#   ## VSAN
#   # vsan_metric_include = [] ## if omitted or empty, all metrics are collected
#   # vsan_metric_exclude = [ "*" ] ## vSAN are not collected by default.
#   ## Whether to skip verifying vSAN metrics against the ones from GetSupportedEntityTypes API.
#   # vsan_metric_skip_verify = false ## false by default.
#
#   ## Interval for sampling vSAN performance metrics, can be reduced down to
#   ## 30 seconds for vSAN 8 U1.
#   # vsan_interval = "5m"
#
#   ## Plugin Settings
#   ## separator character to use for measurement and field names (default: "_")
#   # separator = "_"
#
#   ## number of objects to retrieve per query for realtime resources (vms and hosts)
#   ## set to 64 for vCenter 5.5 and 6.0 (default: 256)
#   # max_query_objects = 256
#
#   ## number of metrics to retrieve per query for non-realtime resources (clusters and datastores)
#   ## set to 64 for vCenter 5.5 and 6.0 (default: 256)
#   # max_query_metrics = 256
#
#   ## number of go routines to use for collection and discovery of objects and metrics
#   # collect_concurrency = 1
#   # discover_concurrency = 1
#
#   ## the interval before (re)discovering objects subject to metrics collection (default: 300s)
#   # object_discovery_interval = "300s"
#
#   ## timeout applies to any of the api request made to vcenter
#   # timeout = "60s"
#
#   ## When set to true, all samples are sent as integers. This makes the output
#   ## data types backwards compatible with Telegraf 1.9 or lower. Normally all
#   ## samples from vCenter, with the exception of percentages, are integer
#   ## values, but under some conditions, some averaging takes place internally in
#   ## the plugin. Setting this flag to "false" will send values as floats to
#   ## preserve the full precision when averaging takes place.
#   # use_int_samples = true
#
#   ## Custom attributes from vCenter can be very useful for queries in order to slice the
#   ## metrics along different dimension and for forming ad-hoc relationships. They are disabled
#   ## by default, since they can add a considerable amount of tags to the resulting metrics. To
#   ## enable, simply set custom_attribute_exclude to [] (empty set) and use custom_attribute_include
#   ## to select the attributes you want to include.
#   ## By default, since they can add a considerable amount of tags to the resulting metrics. To
#   ## enable, simply set custom_attribute_exclude to [] (empty set) and use custom_attribute_include
#   ## to select the attributes you want to include.
#   # custom_attribute_include = []
#   # custom_attribute_exclude = ["*"]
#
#   ## The number of vSphere 5 minute metric collection cycles to look back for non-realtime metrics. In
#   ## some versions (6.7, 7.0 and possible more), certain metrics, such as cluster metrics, may be reported
#   ## with a significant delay (>30min). If this happens, try increasing this number. Please note that increasing
#   ## it too much may cause performance issues.
#   # metric_lookback = 3
#
#   ## Optional SSL Config
#   # ssl_ca = "/path/to/cafile"
#   # ssl_cert = "/path/to/certfile"
#   # ssl_key = "/path/to/keyfile"
#   ## Use SSL but skip chain & host verification
#   # insecure_skip_verify = false
#
#   ## The Historical Interval value must match EXACTLY the interval in the daily
#   # "Interval Duration" found on the VCenter server under Configure > General > Statistics > Statistic intervals
#   # historical_interval = "5m"
#
#   ## Specifies plugin behavior regarding disconnected servers
#   ## Available choices :
#   ##   - error: telegraf will return an error on startup if one the servers is unreachable
#   ##   - ignore: telegraf will ignore unreachable servers on both startup and gather
#   # disconnected_servers_behavior = "error"
#
#   ## HTTP Proxy support
#   # use_system_proxy = true
#   # http_proxy_url = ""


# # A Webhooks Event collector
# [[inputs.webhooks]]
#   ## Address and port to host Webhook listener on
#   service_address = ":1619"
#
#   ## Maximum duration before timing out read of the request
#   # read_timeout = "10s"
#   ## Maximum duration before timing out write of the response
#   # write_timeout = "10s"
#
#   [inputs.webhooks.filestack]
#     path = "/filestack"
#
#     ## HTTP basic auth
#     #username = ""
#     #password = ""
#
#   [inputs.webhooks.github]
#     path = "/github"
#     # secret = ""
#
#     ## HTTP basic auth
#     #username = ""
#     #password = ""
#
#   [inputs.webhooks.mandrill]
#     path = "/mandrill"
#
#     ## HTTP basic auth
#     #username = ""
#     #password = ""
#
#   [inputs.webhooks.rollbar]
#     path = "/rollbar"
#
#     ## HTTP basic auth
#     #username = ""
#     #password = ""
#
#   [inputs.webhooks.papertrail]
#     path = "/papertrail"
#
#     ## HTTP basic auth
#     #username = ""
#     #password = ""
#
#   [inputs.webhooks.particle]
#     path = "/particle"
#
#     ## HTTP basic auth
#     #username = ""
#     #password = ""
#
#   [inputs.webhooks.artifactory]
#     path = "/artifactory"


# # Gather data from a Zipkin server including trace and timing data
# [[inputs.zipkin]]
#   ## URL path for span data
#   # path = "/api/v1/spans"
#
#   ## Port on which Telegraf listens
#   # port = 9411
#
#   ## Maximum duration before timing out read of the request
#   # read_timeout = "10s"
#   ## Maximum duration before timing out write of the response
#   # write_timeout = "10s"
